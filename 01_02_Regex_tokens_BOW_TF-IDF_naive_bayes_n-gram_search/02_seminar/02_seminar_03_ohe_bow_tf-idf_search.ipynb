{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8e206e-195f-4c99-a742-f98f184a9c0a",
   "metadata": {},
   "source": [
    "# One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9455a1de-a29e-485e-aa03-ad2b80114149",
   "metadata": {},
   "source": [
    "Кодирование \"one-hot\" (однозначное кодирование) - это метод представления категориальных данных, таких как слова, символы или другие элементы, в виде бинарных векторов, где каждый элемент вектора соответствует одному элементу категории. В контексте обработки естественного языка (NLP), one-hot encoding используется для представления слов, символов или других элементов текста.\n",
    "\n",
    "Принцип работы one-hot encoding:\n",
    "\n",
    "1. Создается словарь (набор всех уникальных слов или символов), который будет использоваться для кодирования текста.\n",
    "\n",
    "2. Для каждого уникального элемента из словаря создается бинарный вектор фиксированной длины, где все элементы инициализируются нулями.\n",
    "\n",
    "3. Для каждого слова, символа или элемента текста создается соответствующий вектор, и только элемент, соответствующий этому слову или символу в словаре, устанавливается в 1, остальные элементы остаются равными 0.\n",
    "\n",
    "Пример one-hot encoding в NLP:\n",
    "\n",
    "Допустим, у нас есть словарь из следующих слов: [\"я\", \"люблю\", \"NLP\"]. Если у нас есть фраза \"Я люблю NLP\", то one-hot encoding для этой фразы будет выглядеть следующим образом:\n",
    "\n",
    "- \"я\" будет представлено как [1, 0, 0] (первый элемент вектора равен 1, остальные равны 0).\n",
    "- \"люблю\" будет представлено как [0, 1, 0].\n",
    "- \"NLP\" будет представлено как [0, 0, 1].\n",
    "\n",
    "По сути, каждое слово, символ или элемент в тексте кодируется в виде уникального бинарного вектора, что позволяет компьютеру работать с текстовыми данными, представляя их в числовой форме. Однако one-hot encoding имеет ограничения, особенно в случае больших словарей, так как размер векторов может стать очень большим, что может привести к проблемам с вычислительной эффективностью. Тем не менее, это полезный базовый метод для начала работы с текстовыми данными в NLP.\n",
    "\n",
    "\r\n",
    "Преимущества One-Hot Encoding в NLP:\r\n",
    "\r\n",
    "1. **Простота понимания и применения**: One-hot encoding - это простой метод, который легко понимать и реализовывать. Он не требует сложных математических операций и подходит для начинающих в области NLP.\r\n",
    "\r\n",
    "2. **Интерпретируемость**: One-hot векторы явно представляют наличие или отсутствие каждого элемента словаря, что делает их интерпретируемыми.\r\n",
    "\r\n",
    "3. **Инвариантность к порядку**: Порядок слов в тексте не имеет значения при использовании one-hot encoding, потому что каждое слово представлено отдельным бинарным вектором.\r\n",
    "\r\n",
    "Недостатки One-Hot Encoding в NLP:\r\n",
    "\r\n",
    "1. **Размерность векторов**: One-hot encoding приводит к большой размерности векторов, особенно если словарь (набор уникальных слов или символов) большой. Это может привести к высокой вычислительной нагрузке и использованию большого объема памяти.\r\n",
    "\r\n",
    "2. **Отсутствие семантической информации**: Векторы one-hot не содержат семантической информации о словах. Они не учитывают сходство между словами, что может снижать качество некоторых NLP-задач, таких как машинный перевод и анализ тональности.\r\n",
    "\r\n",
    "3. **Разреженность данных**: Векторы one-hot обычно являются разреженными, так как большинство элементов вектора равны нулю. Это увеличивает сложность хранения и обработки данных.\r\n",
    "\r\n",
    "4. **Невозможность обобщения на новые данные**: Если встречается новое слово, которого нет в исходном словаре, его нельзя представить в виде one-hot вектора, что может создать проблемы при обработке неизвестных слов.\r\n",
    "\r\n",
    "5. **Не учитывает смысловые отношения**: One-hot encoding не учитывает семантические отношения между словами, такие как синонимы или антонимы.\r\n",
    "\r\n",
    "Для решения некоторых из этих недостатков могут быть использованы более продвинутые методы кодирования, такие как векторные представления слов (word embeddings), включая Word2Vec, GloVe и FastText, которые способны учить семантические отношения между словами и снижать размерность данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cf53bcb-a8d3-4a82-b1c4-8b839c2815f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "я: [0 0 0 0 1]\n",
      "люблю: [0 0 0 1 0]\n",
      "NLP: [1 0 0 0 0]\n",
      "в: [0 0 1 0 0]\n",
      "Python: [0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Пример списка слов\n",
    "word_list = [\"я\", \"люблю\", \"NLP\", \"в\", \"Python\"]\n",
    "\n",
    "# Создаем объект OneHotEncoder\n",
    "encoder = LabelBinarizer()\n",
    "\n",
    "# Преобразуем список слов в бинарные векторы\n",
    "one_hot_vectors = encoder.fit_transform(word_list)\n",
    "\n",
    "# Выводим результат\n",
    "for word, one_hot_vector in zip(word_list, one_hot_vectors):\n",
    "    print(f\"{word}: {one_hot_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e435790-94ba-4cd0-9228-e628162d5428",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8087d1-7ea7-40de-a5e0-88c2e279f1a9",
   "metadata": {},
   "source": [
    "Модель \"мешок слов\" (Bag of Words, BoW) - это один из базовых методов представления текстовых данных в обработке естественного языка (NLP). Она представляет текст как неупорядоченный набор слов, игнорируя порядок слов и учитывая только их вхождение в текст. Эта концепция имеет несколько ключевых аспектов:\r\n",
    "\r\n",
    "1. **Игнорирование порядка слов**: Модель BoW не учитывает порядок слов в тексте. Она считает, что структура и порядок слов не имеют значения и что каждое слово является независимым от остальных.\r\n",
    "\r\n",
    "2. **Создание словаря**: Прежде чем использовать модель BoW, необходимо создать словарь всех уникальных слов, которые могут встречаться в текстах. Этот словарь будет использоваться для создания векторов признаков.\r\n",
    "\r\n",
    "3. **Подсчет вхождений слов**: Для каждого текста в корпусе подсчитывается, сколько раз каждое слово из словаря встречается в данном тексте.\r\n",
    "\r\n",
    "4. **Представление текстов в виде векторов**: Каждый текст представляется в виде вектора фиксированной длины, где каждый элемент вектора соответствует слову из словаря, а его значение - количеству вхождений этого слова в текст.\r\n",
    "\r\n",
    "Пример:\r\n",
    "Предположим, у нас есть следующие тексты:\r\n",
    "- \"Я люблю программирование.\"\r\n",
    "- \"Программирование интересно.\"\r\n",
    "- \"Программирование - это будущее.\"\r\n",
    "\r\n",
    "Сначала создается словарь всех уникальных слов: [\"Я\", \"люблю\", \"программирование\", \"интересно\", \"это\", \"будущее\"].\r\n",
    "\r\n",
    "Затем каждый текст представляется в виде вектора BoW:\r\n",
    "- \"Я люблю программирование.\" станет [1, 1, 1, 0, 0, 0].\r\n",
    "- \"Программирование интересно.\" станет [0, 0, 1, 1, 0, 0].\r\n",
    "- \"Программирование - это будущее.\" станет [0, 0, 1, 0, 1, 1].\r\n",
    "\r\n",
    "Преимущества и недостатки модели BoW:\r\n",
    "\r\n",
    "Преимущества:\r\n",
    "- Простота и понятность.\r\n",
    "- Эффективность для многих задач, таких как классификация текстов.\r\n",
    "- Позволяет работать с текстами в числовой форме, что необходимо для многих алгоритмов машинного обучения.\r\n",
    "\r\n",
    "Недостатки:\r\n",
    "- Не учитывает семантические отношения между словами.\r\n",
    "- Разреженность данных: векторы BoW часто имеют много нулей, что увеличивает вычислительную сложность.\r\n",
    "- Потеря информации о порядке слов.\r\n",
    "- Не подходит для задач, где важен контекст и семантика, таких как машинный перевод.\r\n",
    "\r\n",
    "Модель BoW является важным концептуальным инструментом в NLP и обычно используется как базовая модель для более сложных методов, таких как TF-IDF и векторные представления слов (Word Embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28aebfef-bc80-4891-ad65-f3a29f4b06ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица Bag of Words:\n",
      "[[0 0 0 0 0 0 0 1 0 1 0 1]\n",
      " [0 0 1 0 0 0 1 0 1 0 0 1]\n",
      " [1 1 0 1 1 1 0 0 0 0 1 0]]\n",
      "\n",
      "Список слов:\n",
      "['быть' 'длинными' 'еще' 'или' 'короткими' 'могут' 'один' 'пример' 'текст'\n",
      " 'текста' 'тексты' 'это']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текста 1.\",\n",
    "    \"Это еще один текст.\",\n",
    "    \"Тексты могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Создаем объект CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Преобразуем корпус в матрицу Bag of Words\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Получаем список слов (терминов)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим матрицу Bag of Words\n",
    "print(\"Матрица Bag of Words:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Выводим список слов\n",
    "print(\"\\nСписок слов:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f2d88-783b-4e62-a3a4-81300b85d22e",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e93c833-ed5c-406e-8c0a-130004bc007e",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency) - это статистическая мера, используемая в обработке естественного языка (NLP) для оценки важности слова в документе относительно корпуса документов. TF-IDF представляет собой комбинацию двух компонентов:\n",
    "\n",
    "1. **TF (Term Frequency)** - Частота термина: это мера, которая оценивает, насколько часто слово встречается в данном документе. Она измеряет, насколько слово важно для конкретного документа.\n",
    "\n",
    "2. **IDF (Inverse Document Frequency)** - Обратная частота документа: это мера, которая оценивает, насколько слово уникально для всего корпуса документов. Она измеряет, насколько слово важно в контексте всего корпуса.\n",
    "\n",
    "Комбинируя TF и IDF, мы можем вычислить TF-IDF для каждого слова в документе. Формула для вычисления TF-IDF для слова `w` в документе `d` обычно выглядит следующим образом:\n",
    "\n",
    "```\n",
    "TF-IDF(w, d) = TF(w, d) * IDF(w)\n",
    "```\n",
    "\n",
    "Где:\n",
    "- `TF(w, d)` - Частота термина (Term Frequency) слова `w` в документе `d`. Это может быть просто количество вхождений слова в документе или нормализованное значение, например, частота встречаемости слова в документе относительно общей длины документа.\n",
    "- `IDF(w)` - Обратная частота документа (Inverse Document Frequency) слова `w`. Это значение вычисляется как логарифм обратного отношения общего числа документов в корпусе к числу документов, содержащих слово `w`. Формула может быть слегка варьирована для учета различных вариантов.\n",
    "\n",
    "Преимущества TF-IDF в NLP:\n",
    "- Помогает выделить важные слова в документах.\n",
    "- Учитывает частоту и уникальность слова.\n",
    "- Хорошо работает для выявления ключевых слов и тематического моделирования.\n",
    "- Используется для ранжирования документов в поисковых системах.\n",
    "\n",
    "Недостатки TF-IDF в NLP:\n",
    "- Не учитывает семантику слов.\n",
    "- Не учитывает порядок слов в тексте.\n",
    "- Не обрабатывает синонимы и многозначные слова.\n",
    "\n",
    "TF-IDF - это полезная мера для анализа и обработки текстовых данных, и она часто используется в задачах информационного поиска, классификации текстов и кластеризации документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f33c61-8313-4fd0-ad38-1f0bd3216953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Матрица:\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.62276601 0.         0.62276601 0.         0.4736296 ]\n",
      " [0.         0.         0.52863461 0.         0.         0.\n",
      "  0.52863461 0.         0.52863461 0.         0.         0.40204024]\n",
      " [0.40824829 0.40824829 0.         0.40824829 0.40824829 0.40824829\n",
      "  0.         0.         0.         0.         0.40824829 0.        ]]\n",
      "\n",
      "Список слов:\n",
      "['быть' 'длинными' 'еще' 'или' 'короткими' 'могут' 'один' 'пример' 'текст'\n",
      " 'текста' 'тексты' 'это']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текста 1.\",\n",
    "    \"Это еще один текст.\",\n",
    "    \"Тексты могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Создаем объект TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Преобразуем корпус в TF-IDF матрицу\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Получаем список слов (терминов)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим TF-IDF матрицу\n",
    "print(\"TF-IDF Матрица:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Выводим список слов\n",
    "print(\"\\nСписок слов:\")\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45da8a5-e080-4171-a6d0-fd74dcb4b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score модели: 0.5937747194432215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avglinsky\\PycharmProjects\\nlp-course-timeline\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Загружаем набор данных \"20 Newsgroups\"\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Разделяем данные на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Создаем TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Преобразуем текст в векторы TF-IDF\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Создаем и обучаем модель логистической регрессии\n",
    "logistic_regression = LogisticRegression(max_iter=10, random_state=42)\n",
    "logistic_regression.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Предсказываем классы для тестового набора данных\n",
    "y_pred = logistic_regression.predict(X_test_tfidf)\n",
    "\n",
    "# Оцениваем модель с использованием метрики F1-score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1-score модели: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490fc2a-0b58-454b-8550-8bce13d62d1d",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e6ce4-f930-4b1c-b708-e7c525b1136a",
   "metadata": {},
   "source": [
    "BM25 (Best Matching 25) - это вероятностная модель ранжирования для информационного поиска и анализа текста. BM25 является улучшенной версией более ранней модели BM25, которая была представлена в 1994 году. BM25 используется для оценки релевантности документов по отношению к запросу пользователя в системах поиска, включая поисковые движки, и для анализа текстовых данных.\r\n",
    "\r\n",
    "Основные концепции BM25:\r\n",
    "\r\n",
    "1. **Веса терминов**: BM25 учитывает веса терминов (слов) в запросе и документе. Веса терминов определяют, насколько важен каждый термин для релевантности документа запросу.\r\n",
    "\r\n",
    "2. **Длина документа**: BM25 учитывает длину документа. Это означает, что более длинные документы могут иметь более низкие оценки релевантности, если они содержат много терминов, которые не совпадают с запросом.\r\n",
    "\r\n",
    "3. **IDF (Inverse Document Frequency)**: BM25 использует обратную документную частоту (IDF) для учета уникальности терминов в корпусе документов. Термины, которые встречаются редко в корпусе, имеют более высокие веса.\r\n",
    "\r\n",
    "4. **Параметры ранжирования**: BM25 имеет несколько параметров ранжирования, таких как параметр \"k\" и параметр \"b\", которые можно настроить под конкретные задачи и типы данных.\r\n",
    "\r\n",
    "5. **Вероятностное ранжирование**: BM25 представляет собой вероятностную модель ранжирования, которая оценивает вероятность, что документ будет релевантным для запроса.\r\n",
    "\r\n",
    "Преимущества BM25 в NLP:\r\n",
    "\r\n",
    "- BM25 является одним из наиболее эффективных алгоритмов ранжирования для поисковых систем.\r\n",
    "- Он хорошо работает с короткими и длинными текстами.\r\n",
    "- Учитывает уникальность и веса терминов, что делает его более точным по сравнению с некоторыми более простыми моделями.\r\n",
    "\r\n",
    "Недостатки BM25 в NLP:\r\n",
    "\r\n",
    "- Требует настройки параметров для оптимальной производительности.\r\n",
    "- Может быть вычислительно затратным при обработке больших корпусов документов.\r\n",
    "- Не учитывает семантические отношения между словами.\r\n",
    "\r\n",
    "BM25 широко используется в поисковых системах и информационном поиске для ранжирования и выдачи наиболее релевантных документов для пользовательских запросов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec68027-5753-448a-986a-fbcf36806a51",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\frac{{\\text{IDF}(q_i) \\cdot f(q_i, D) \\cdot (k_1 + 1)}}{{f(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{{\\text{dl}(D)}}{{\\text{avgdl}}}\\right)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7623a67-2d29-4518-a996-4ba86cacc668",
   "metadata": {},
   "source": [
    "Формула BM25 (Best Matching 25)\r",
    "ормуле:\r\n",
    "\r\n",
    "- BM25(D, Q) - значение BM25 для документа D и запроса Q.\r\n",
    "- n - количество уникальных слов в запросе Q.\r\n",
    "- q_i - i-е уникальное слово в запросе Q.\r\n",
    "- IDF(q_i) - обратная документная частота (inverse document frequency) для слова q_i.\r\n",
    "- f(q_i, D) - частота слова q_i в документе D.\r\n",
    "- k_1 - параметр, обычно выбирается в диапазоне от 1.2 до 2.0.\r\n",
    "- b - параметр, обычно выбирается в диапазоне от 0.0 до 1.0.\r\n",
    "- dl(D) - длина документа D (количество слов в документе).\r\n",
    "- avgdl - средняя длина документа в коллекции.\r\n",
    "\r\n",
    "Вы можете вставить эту формулу в свой LaTeX-документ, обеспечив правильное форматирование и использование необходимых пакетов для математических выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dd8af1-2d21-47e5-8d4d-45373412e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Документ 1(Это пример текст 1.): BM25 Score = 0.598\n",
      "Документ 2(Это еще один текст): BM25 Score = 0.052\n",
      "Документ 3(текст могут быть короткими или длинными.): BM25 Score = 0.043\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Пример корпуса текстов\n",
    "corpus = [\n",
    "    \"Это пример текст 1.\",\n",
    "    \"Это еще один текст\",\n",
    "    \"текст могут быть короткими или длинными.\"\n",
    "]\n",
    "\n",
    "# Разделите тексты на слова (токенизация)\n",
    "tokenized_corpus = [text.split() for text in corpus]\n",
    "\n",
    "# Создайте модель BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Запрос\n",
    "query = \"пример текст\"\n",
    "\n",
    "# Токенизируйте запрос\n",
    "query_tokens = query.split()\n",
    "\n",
    "# Вычислите BM25 для запроса и корпуса\n",
    "scores = bm25.get_scores(query_tokens)\n",
    "\n",
    "# Выведите результаты (релевантность документов по запросу)\n",
    "for i, score in enumerate(scores):\n",
    "    print(f\"Документ {i + 1}({corpus[i]}): BM25 Score = {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35f6fc-12ff-4b13-8cf7-de2bca5fb37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
